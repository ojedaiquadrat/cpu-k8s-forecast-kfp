{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8005bf7e-88e6-4c88-9f37-49d4e24f0b79",
   "metadata": {},
   "source": [
    "## Non-overlapping batches of 24 consecutive samples, 10 seconds rate for reading values, i.e.:\n",
    "\n",
    "First batch: t1-t24,\n",
    "\n",
    "Second batch: t25-t48,\n",
    "\n",
    "Third batch: t49-t72,\n",
    "\n",
    "etc.\n",
    "\n",
    "Let’s build a modular producer that:\n",
    "\n",
    "Scrapes Prometheus every 10 seconds (configurable).\n",
    "\n",
    "Collects exactly 24 samples per batch (configurable).\n",
    "\n",
    "After collecting a batch, publishes the whole batch as one Kafka message (instead of individual points).\n",
    "\n",
    "Can be customized via ENV or function args.\n",
    "\n",
    "Each time it publishes a batch, prints it as well for your checks.\n",
    "\n",
    "**key features:**\n",
    "\n",
    "No buffer per instance, no leftovers:\n",
    "Each time, it collects 24 new points, sends, and then starts the next batch cleanly.\n",
    "\n",
    "Prints each point as it’s scraped:\n",
    "Like [01] 2025-09-18 19:52:40, 33.17233333\n",
    "\n",
    "Batch printout:\n",
    "Each time it publishes a batch, prints all 24 with nice formatting.\n",
    "\n",
    "No overlap, no sliding, no duplication.\n",
    "\n",
    "Always starts fresh:\n",
    "Run script multiple times, each run collects new data starting from now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f192771d-0fbf-44b0-8f05-b31310175219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install prometheus-api-client kafka-python\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import datetime\n",
    "from typing import List\n",
    "from prometheus_api_client import PrometheusConnect\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "def fresh_batches_and_publish(\n",
    "    prom_url: str = os.getenv(\"PROM_URL\", \"http://prometheus-operated.prometheus.svc.cluster.local:9090\"),\n",
    "    kafka_bootstrap: str = os.getenv(\"KAFKA_BOOTSTRAP_SERVERS\", \"kafka.apache-kafka.svc.cluster.local:9092\"),\n",
    "    topic: str = os.getenv(\"KAFKA_TOPIC\", \"cpu-batch\"),\n",
    "    prom_query: str = os.getenv(\n",
    "        \"PROM_QUERY\",\n",
    "        'sum by (instance) (rate(node_cpu_seconds_total{mode!=\"idle\"}[1m]))'\n",
    "    ),\n",
    "    target_instance: str = os.getenv(\"TARGET_INSTANCE\", None),   # e.g. \"172.19.0.3:9100\"\n",
    "    batch_size: int = int(os.getenv(\"BATCH_SIZE\", \"24\")),\n",
    "    interval_sec: int = int(os.getenv(\"INTERVAL_SEC\", \"10\")),\n",
    "    print_each_batch: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Collect exactly batch_size points for each batch, publish, repeat. Start fresh every run.\n",
    "    Runs infinitely!\n",
    "    \"\"\"\n",
    "    prom = PrometheusConnect(url=prom_url, disable_ssl=True)\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers=[s.strip() for s in kafka_bootstrap.split(\",\") if s.strip()],\n",
    "        value_serializer=lambda v: json.dumps(v).encode(\"utf-8\"),\n",
    "    )\n",
    "\n",
    "    print(\"============ CONFIG ============\")\n",
    "    print(f\"Prometheus: {prom_url}\")\n",
    "    print(f\"Query:      {prom_query}\")\n",
    "    print(f\"Instance:   {target_instance if target_instance else 'ALL'}\")\n",
    "    print(f\"Kafka:      {kafka_bootstrap}\")\n",
    "    print(f\"Topic:      {topic}\")\n",
    "    print(f\"Interval:   {interval_sec} sec\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(\"================================\\n\")\n",
    "\n",
    "    batches_sent = 0\n",
    "\n",
    "    while True:\n",
    "        batch_points: List[dict] = []\n",
    "        print(f\"[producer] Collecting batch #{batches_sent+1} ...\")\n",
    "        while len(batch_points) < batch_size:\n",
    "            tick_ts = int(time.time())\n",
    "            try:\n",
    "                result = prom.custom_query(query=prom_query)\n",
    "            except Exception as e:\n",
    "                print(f\"[producer] Prometheus query FAILED: {e}\")\n",
    "                time.sleep(interval_sec)\n",
    "                continue\n",
    "\n",
    "            found = False\n",
    "            for sample in result:\n",
    "                inst = sample[\"metric\"].get(\"instance\")\n",
    "                if target_instance and inst != target_instance:\n",
    "                    continue\n",
    "                cpu_val = float(sample[\"value\"][1])\n",
    "                ts_human = datetime.datetime.fromtimestamp(tick_ts).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                print(f\"[{len(batch_points)+1:02d}] {ts_human}, {cpu_val:.8f}\")\n",
    "                batch_points.append({\n",
    "                    \"ts\": ts_human,\n",
    "                    \"cpu_pct\": cpu_val,\n",
    "                })\n",
    "                found = True\n",
    "                break  # only take first match for target_instance\n",
    "\n",
    "            if not found:\n",
    "                print(\"[producer] No data for instance, retrying...\")\n",
    "            if len(batch_points) < batch_size:\n",
    "                time.sleep(interval_sec)  # wait for next point\n",
    "\n",
    "        # Build message for Kafka\n",
    "        msg = {\n",
    "            \"instance\": target_instance,\n",
    "            \"batch_ts\": [pt[\"ts\"] for pt in batch_points],\n",
    "            \"batch_cpu_pct\": [pt[\"cpu_pct\"] for pt in batch_points],\n",
    "            \"batch_number\": batches_sent + 1,\n",
    "        }\n",
    "        producer.send(topic, value=msg)\n",
    "        producer.flush()\n",
    "        if print_each_batch:\n",
    "            print(f\"\\n[producer] Published batch #{batches_sent+1} for {target_instance}:\")\n",
    "            for i, (ts, v) in enumerate(zip(msg[\"batch_ts\"], msg[\"batch_cpu_pct\"]), 1):\n",
    "                print(f\"  [{i:02d}] {ts}, {v:.8f}\")\n",
    "            print(\"-\" * 40)\n",
    "        batches_sent += 1\n",
    "\n",
    "        # --- Fix: Wait for next *new* interval before starting the next batch ---\n",
    "        time.sleep(interval_sec)\n",
    "\n",
    "    # (never exits, runs forever)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f72021-6792-4289-9ce2-81494ef986dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fresh_batches_and_publish(\n",
    "    prom_url=\"http://prometheus-operated.prometheus.svc.cluster.local:9090\",\n",
    "    kafka_bootstrap=\"kafka.apache-kafka.svc.cluster.local:9092\",\n",
    "    topic=\"cpu-batch\",\n",
    "    prom_query='sum by (instance) (rate(node_cpu_seconds_total{mode!=\"idle\"}[1m]))',\n",
    "    target_instance=\"172.19.0.3:9100\",  # your node\n",
    "    batch_size=24,\n",
    "    interval_sec=10,\n",
    "    print_each_batch=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298ad7ec-420c-42cc-85ca-43a720afc673",
   "metadata": {},
   "source": [
    "### V2\n",
    "\n",
    "**More robust:** track the last timestamp used, and always wait for the next real new point).\n",
    "tracking the last timestamp used and always waiting for the next new point is the most robust and accurate solution, especially in distributed or cloud settings\n",
    "\n",
    "**Why “wait for new timestamp” is more robust:**\n",
    "\n",
    "Network delays or Prometheus lag:\n",
    "If you just sleep(interval_sec), the next point you get may have the same timestamp as before (especially if the scrape interval, query interval, or system clock drifts).\n",
    "\n",
    "System time may not be perfectly in sync\n",
    "If you run this for a long time, clocks can slip, and you might see repeated values.\n",
    "\n",
    "You want each batch to have strictly unique, increasing timestamps.\n",
    "\n",
    "This also handles the case where you miss a point (Prometheus slow, or system hiccup):\n",
    "You’ll wait until a truly new value arrives, and then record it—so your batches always have 24 unique timestamps.\n",
    "\n",
    "**How to implement:**\n",
    "\n",
    "Keep track of the timestamp of the last collected point.\n",
    "\n",
    "In each loop, after you fetch from Prometheus,\n",
    "\n",
    "If the timestamp is different from the previous one, accept and store it.\n",
    "\n",
    "If not, sleep a bit and try again (until you see a new timestamp).\n",
    "\n",
    "Result: You’ll never collect two points with the same timestamp, even if Prometheus scrapes are delayed or your code resumes early\n",
    "\n",
    "**Advantages of this approach:**\n",
    "\n",
    "No duplicate points ever, regardless of query speed or Prometheus delays.\n",
    "\n",
    "Handles missing data: waits until new point is available, no assumption about timing.\n",
    "\n",
    "Maximum data integrity for downstream ML use.\n",
    "\n",
    "**How this works:**\n",
    "\n",
    "It only accepts a new sample when the Prometheus timestamp is exactly a multiple of your interval (interval_sec).\n",
    "\n",
    "If you want 10s intervals, you'll only get times like 20:10:00, 20:10:10, 20:10:20, etc.\n",
    "\n",
    "No duplicates: it skips timestamps it already saw.\n",
    "\n",
    "Runs forever, producing one clean batch after another.\n",
    "\n",
    "No sleep for 10s needed; it waits for the next aligned data point automatically.\n",
    "\n",
    "If your Prometheus is \"late\" or \"skips\" a sample, you'll just have a bigger gap (never duplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1daa6db1-98bf-4efd-8798-0fbb45cbdd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install prometheus-api-client kafka-python\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import datetime\n",
    "from typing import List\n",
    "\n",
    "from prometheus_api_client import PrometheusConnect\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "def fresh_batches_and_publish(\n",
    "    prom_url: str = os.getenv(\"PROM_URL\", \"http://prometheus-operated.prometheus.svc.cluster.local:9090\"),\n",
    "    kafka_bootstrap: str = os.getenv(\"KAFKA_BOOTSTRAP_SERVERS\", \"kafka.apache-kafka.svc.cluster.local:9092\"),\n",
    "    topic: str = os.getenv(\"KAFKA_TOPIC\", \"cpu-batch\"),\n",
    "    prom_query: str = os.getenv(\n",
    "        \"PROM_QUERY\",\n",
    "        'sum by (instance) (rate(node_cpu_seconds_total{mode!=\"idle\"}[1m]))'\n",
    "    ),\n",
    "    target_instance: str = os.getenv(\"TARGET_INSTANCE\", None),\n",
    "    batch_size: int = int(os.getenv(\"BATCH_SIZE\", \"24\")),\n",
    "    interval_sec: int = int(os.getenv(\"INTERVAL_SEC\", \"10\")),\n",
    "    print_each_batch: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Collects only points where the timestamp is aligned (multiple of interval_sec).\n",
    "    Ensures precise 10s (or any interval) spacing for your ML pipeline.\n",
    "    Runs infinitely!\n",
    "    \"\"\"\n",
    "    prom = PrometheusConnect(url=prom_url, disable_ssl=True)\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers=[s.strip() for s in kafka_bootstrap.split(\",\") if s.strip()],\n",
    "        value_serializer=lambda v: json.dumps(v).encode(\"utf-8\"),\n",
    "    )\n",
    "\n",
    "    print(\"============ CONFIG ============\")\n",
    "    print(f\"Prometheus: {prom_url}\")\n",
    "    print(f\"Query:      {prom_query}\")\n",
    "    print(f\"Instance:   {target_instance if target_instance else 'ALL'}\")\n",
    "    print(f\"Kafka:      {kafka_bootstrap}\")\n",
    "    print(f\"Topic:      {topic}\")\n",
    "    print(f\"Interval:   {interval_sec} sec\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(\"================================\\n\")\n",
    "\n",
    "    batches_sent = 0\n",
    "    last_seen_ts = None\n",
    "\n",
    "    while True:\n",
    "        batch_points: List[dict] = []\n",
    "        print(f\"[producer] Collecting batch #{batches_sent+1} ...\")\n",
    "        while len(batch_points) < batch_size:\n",
    "            try:\n",
    "                result = prom.custom_query(query=prom_query)\n",
    "            except Exception as e:\n",
    "                print(f\"[producer] Prometheus query FAILED: {e}\")\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "\n",
    "            found = False\n",
    "            for sample in result:\n",
    "                inst = sample[\"metric\"].get(\"instance\")\n",
    "                if target_instance and inst != target_instance:\n",
    "                    continue\n",
    "                cpu_val = float(sample[\"value\"][1])\n",
    "                prom_ts = int(float(sample[\"value\"][0]))\n",
    "                # Only accept aligned timestamps (e.g., every 10 seconds: :00, :10, :20, ...)\n",
    "                if prom_ts % interval_sec != 0:\n",
    "                    continue\n",
    "                # Skip duplicate timestamps\n",
    "                if last_seen_ts is not None and prom_ts == last_seen_ts:\n",
    "                    continue\n",
    "                ts_human = datetime.datetime.fromtimestamp(prom_ts).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                print(f\"[{len(batch_points)+1:02d}] {ts_human}, {cpu_val:.8f}\")\n",
    "                batch_points.append({\n",
    "                    \"ts\": ts_human,\n",
    "                    \"cpu_pct\": cpu_val,\n",
    "                })\n",
    "                last_seen_ts = prom_ts\n",
    "                found = True\n",
    "                break  # take only one sample per query (for target_instance)\n",
    "\n",
    "            if not found or (len(batch_points) < batch_size):\n",
    "                time.sleep(1)  # Check again soon\n",
    "\n",
    "        msg = {\n",
    "            \"instance\": target_instance,\n",
    "            \"batch_ts\": [pt[\"ts\"] for pt in batch_points],\n",
    "            \"batch_cpu_pct\": [pt[\"cpu_pct\"] for pt in batch_points],\n",
    "            \"batch_number\": batches_sent + 1,\n",
    "        }\n",
    "        producer.send(topic, value=msg)\n",
    "        producer.flush()\n",
    "        if print_each_batch:\n",
    "            print(f\"\\n[producer] Published batch #{batches_sent+1} for {target_instance}:\")\n",
    "            for i, (ts, v) in enumerate(zip(msg[\"batch_ts\"], msg[\"batch_cpu_pct\"]), 1):\n",
    "                print(f\"  [{i:02d}] {ts}, {v:.8f}\")\n",
    "            print(\"-\" * 40)\n",
    "        batches_sent += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c96a443-f9d5-4eec-8a42-d0a66e33f4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ CONFIG ============\n",
      "Prometheus: http://prometheus-operated.prometheus.svc.cluster.local:9090\n",
      "Query:      sum by (instance) (rate(node_cpu_seconds_total{mode!=\"idle\"}[1m]))\n",
      "Instance:   172.19.0.3:9100\n",
      "Kafka:      kafka.apache-kafka.svc.cluster.local:9092\n",
      "Topic:      cpu-batch\n",
      "Interval:   10 sec\n",
      "Batch size: 24\n",
      "================================\n",
      "\n",
      "[producer] Collecting batch #1 ...\n",
      "[01] 2025-09-18 20:31:50, 29.61666667\n",
      "[02] 2025-09-18 20:32:00, 29.61666667\n",
      "[03] 2025-09-18 20:32:10, 29.61666667\n",
      "[04] 2025-09-18 20:32:20, 29.53633333\n"
     ]
    }
   ],
   "source": [
    "fresh_batches_and_publish(\n",
    "    prom_url=\"http://prometheus-operated.prometheus.svc.cluster.local:9090\",\n",
    "    kafka_bootstrap=\"kafka.apache-kafka.svc.cluster.local:9092\",\n",
    "    topic=\"cpu-batch\",\n",
    "    prom_query='sum by (instance) (rate(node_cpu_seconds_total{mode!=\"idle\"}[1m]))',\n",
    "    target_instance=\"172.19.0.3:9100\",  # ISI cluster node\n",
    "    batch_size=24,\n",
    "    interval_sec=10,\n",
    "    print_each_batch=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0afc185-1a99-4b9c-8d9e-ddea7ed6cd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
