{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50ae1955-5a44-44e4-a43f-15da17cd0b9d",
   "metadata": {},
   "source": [
    "# Exploratory model analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0c9d728-2d14-452a-b535-f57b521d24fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import io\n",
    "import os\n",
    "from minio import Minio\n",
    "from io import BytesIO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5166254-9ab6-4ee8-bdfb-5287d16613c0",
   "metadata": {},
   "source": [
    "# 1.  Configuration for Local MLflow Tracking and MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d51627d-9f0e-4005-aefc-4e45a00074e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Create Minio Client and Reads Data\"\n",
    "\n",
    "minio_client = Minio(\n",
    "    \"minio-service.kubeflow.svc.cluster.local:9000\",\n",
    "    access_key=\"minio\",\n",
    "    secret_key=\"minio123\",\n",
    "    secure=False,\n",
    "    )\n",
    "\n",
    "# ------------------------\n",
    "# Configuration for MLflow Tracking\n",
    "# ------------------------\n",
    "# --- MLflow Tracking Config ---\n",
    "TRACKING_URI = \"http://sunrise-mlflow-tracking.mlflow.svc.cluster.local:5080\"\n",
    "EXPERIMENT_NAME = \"k8s-cpu-forecasting\"\n",
    "REGISTERED_MODEL_NAME = \"cpu-pct\"\n",
    "\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "client = MlflowClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3667889-2049-4e92-be25-af485019a627",
   "metadata": {},
   "source": [
    "# 2. Early Stopping Utility\n",
    "\n",
    "'''\n",
    "Remembers the best validation loss seen so far.\n",
    "\n",
    "If the current validation loss improves (by at least min_delta[0.01 - 0]), it resets its counter.\n",
    "\n",
    "If not, it increments the counter.\n",
    "\n",
    "If the counter reaches patience[5-10] (number of allowed epochs without improvement), it sets early_stop = True, signaling you should halt training.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e38bd51c-2ba8-4d8d-abcc-5c78579fd69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None or val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205cbf5f-117f-4e8e-af51-154f704ea841",
   "metadata": {},
   "source": [
    "# 3. Model Definition\n",
    "\n",
    "nput: x shape is (batch, seq_len, input_size) (e.g., (32, 5, 1) for 32 samples, window size 5, 1 feature).\n",
    "\n",
    "out, _ = self.lstm(x):\n",
    "\n",
    "Runs the sequence through the LSTM.\n",
    "\n",
    "out contains all hidden states for all timesteps (out.shape == (batch, seq_len, hidden_size)).\n",
    "\n",
    "out[:, -1, :]:\n",
    "\n",
    "Take the hidden state from the last time step for each batch—this encodes the whole sequence's info for forecasting the next step.\n",
    "\n",
    "self.fc(...):\n",
    "\n",
    "Pass the last hidden state through the linear layer to get your forecast.\n",
    "\n",
    "unsqueeze(1):\n",
    "\n",
    "Adds an extra dimension for compatibility (PyTorch expects shape (batch, horizon, features)). If forecasting one step, horizon is 1.\n",
    "\n",
    "           ┌─────────────────────────────────────────────────────────┐\n",
    "           │                  Input Sequence (batch)                 │\n",
    "           │      X: (batch, window_size, input_size)                │\n",
    "           │      Example: [[x₁], [x₂], [x₃], [x₄], [x₅]]           │\n",
    "           └─────────────────────────────────────────────────────────┘\n",
    "                                    │\n",
    "                                    ▼\n",
    "      ┌────────────────────────────────────────────────────────────┐\n",
    "      │                  LSTM Layers (stacked)                    │\n",
    "      │        - num_layers (e.g., 2)                             │\n",
    "      │        - hidden_size (e.g., 64)                           │\n",
    "      │        - processes sequence step by step                   │\n",
    "      └────────────────────────────────────────────────────────────┘\n",
    "                                    │\n",
    "                          (All time steps produce hidden states)\n",
    "                                    │\n",
    "                                    ▼\n",
    "       ┌──────────────────────────────────────────────────────────┐\n",
    "       │     Select LAST hidden state from LSTM output           │\n",
    "       │     out[:, -1, :]  → shape: (batch, hidden_size)        │\n",
    "       └──────────────────────────────────────────────────────────┘\n",
    "                                    │\n",
    "                                    ▼\n",
    "      ┌───────────────────────────────────────────────────────────┐\n",
    "      │    Fully Connected (Linear) Layer                        │\n",
    "      │    - Maps hidden state to output_size                    │\n",
    "      │    - Output: (batch, output_size)                        │\n",
    "      └───────────────────────────────────────────────────────────┘\n",
    "                                    │\n",
    "                                    ▼\n",
    "      ┌───────────────────────────────────────────────────────────┐\n",
    "      │    Unsqueeze to add horizon dimension                    │\n",
    "      │    Output: (batch, 1, output_size)                      │\n",
    "      │    (for 1-step forecasting: horizon=1)                  │\n",
    "      └───────────────────────────────────────────────────────────┘\n",
    "\n",
    "\n",
    "Input Batch (X):                Shape: (32, 5, 1)\n",
    "  └── 32 samples (batch size)\n",
    "  └── 5 time steps (window size)\n",
    "  └── 1 feature (CPU%)\n",
    "\n",
    "           │\n",
    "           ▼\n",
    "─────────────────────────────────────────────\n",
    "         LSTM Layer 1\n",
    "    - Input:  (32, 5, 1)\n",
    "    - Output: (32, 5, 64)\n",
    "─────────────────────────────────────────────\n",
    "           │\n",
    "           ▼\n",
    "─────────────────────────────────────────────\n",
    "         LSTM Layer 2\n",
    "    - Input:  (32, 5, 64)\n",
    "    - Output: (32, 5, 64)\n",
    "─────────────────────────────────────────────\n",
    "           │\n",
    "           ▼\n",
    "Take the last time step's hidden state:\n",
    "    - out[:, -1, :]  →  (32, 64)\n",
    "           │\n",
    "           ▼\n",
    "Fully Connected (Linear) Layer:\n",
    "    - Input:  (32, 64)\n",
    "    - Output: (32, 1)\n",
    "           │\n",
    "           ▼\n",
    "Unsqueeze to add horizon dim:\n",
    "    - Output: (32, 1, 1)\n",
    "\n",
    "\n",
    "### How each part works:\n",
    "\n",
    "Input:\n",
    "Each sample is a sequence of 5 timesteps (window), each timestep has 1 value (CPU%).\n",
    "\n",
    "LSTM Layer 1:\n",
    "Transforms each input to a hidden state of size 64 for each time step.\n",
    "\n",
    "LSTM Layer 2:\n",
    "Takes outputs from Layer 1, produces new hidden state of size 64 (for deeper abstraction).\n",
    "\n",
    "Select Last Time Step:\n",
    "Use only the final hidden state for each sample.\n",
    "\n",
    "Linear Layer:\n",
    "Maps 64 → 1 to make a prediction for the next CPU%.\n",
    "\n",
    "Unsqueeze:\n",
    "Makes output shape (batch, 1, 1) — good for compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c81abf49-4897-4535-93ae-785f90834bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMForecaster(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, output_size=1, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out.unsqueeze(1)  # Shape: (batch, horizon, features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fcb9e2-1525-47af-a0b6-e4b32f0814a8",
   "metadata": {},
   "source": [
    "# Getting my X_train y_train X_val and y-val from MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9266605c-e772-4c12-b751-4dceb6f012f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded from minio://k8s-resources-forecast/data/k8s-preprocessed/node-1-X_train/X_train.npy (shape: (8238, 5, 1))\n",
      "Downloaded from minio://k8s-resources-forecast/data/k8s-preprocessed/node-1-y_train/y_train.npy (shape: (8238, 1, 1))\n",
      "Downloaded from minio://k8s-resources-forecast/data/k8s-preprocessed/node-1-X_test/X_test.npy (shape: (2060, 5, 1))\n",
      "Downloaded from minio://k8s-resources-forecast/data/k8s-preprocessed/node-1-y_test/y_test.npy (shape: (2060, 1, 1))\n",
      "X_train.shape: <class 'numpy.ndarray'>\n",
      "y_train.shape: <class 'numpy.ndarray'>\n",
      "X_val.shape: <class 'numpy.ndarray'>\n",
      "y_test.shape: <class 'numpy.ndarray'> \n",
      "\n",
      "X_train.shape: (8238, 5, 1)\n",
      "y_train.shape: (8238, 1, 1)\n",
      "X_val.shape: (2060, 5, 1)\n",
      "y_val.shape: (2060, 1, 1)\n",
      "\n",
      "First training input window (X_train[0]): [0.02173913 0.02173913 0.02173913 0.02173913 0.02173913]\n",
      "First training target (y_train[0]): [0.02173913]\n",
      "\n",
      "Last training input window (X_train[-1]): [0.22826087 0.19565217 0.22826087 0.25       0.20652174]\n",
      "Last training target (y_train[-1]): [0.22826087]\n",
      "\n",
      "First test input window (X_val[0]): [0.19565217 0.22826087 0.25       0.20652174 0.22826087]\n",
      "First test target (y_val[0]): [0.23913043]\n",
      "\n",
      "Last test input window (X_val[-1]): [0.06521739 0.08695652 0.08695652 0.07608696 0.10869565]\n",
      "Last test target (y_val[-1]): [0.02173913]\n",
      "First 5 input windows (X_val):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.19565217],\n",
       "        [0.22826087],\n",
       "        [0.25      ],\n",
       "        [0.20652174],\n",
       "        [0.22826087]],\n",
       "\n",
       "       [[0.22826087],\n",
       "        [0.25      ],\n",
       "        [0.20652174],\n",
       "        [0.22826087],\n",
       "        [0.23913043]],\n",
       "\n",
       "       [[0.25      ],\n",
       "        [0.20652174],\n",
       "        [0.22826087],\n",
       "        [0.23913043],\n",
       "        [0.22826087]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.09782609],\n",
       "        [0.08695652],\n",
       "        [0.06521739],\n",
       "        [0.08695652],\n",
       "        [0.08695652]],\n",
       "\n",
       "       [[0.08695652],\n",
       "        [0.06521739],\n",
       "        [0.08695652],\n",
       "        [0.08695652],\n",
       "        [0.07608696]],\n",
       "\n",
       "       [[0.06521739],\n",
       "        [0.08695652],\n",
       "        [0.08695652],\n",
       "        [0.07608696],\n",
       "        [0.10869565]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Corresponding targets (y_val):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.23913043]],\n",
       "\n",
       "       [[0.22826087]],\n",
       "\n",
       "       [[0.25      ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.07608696]],\n",
       "\n",
       "       [[0.10869565]],\n",
       "\n",
       "       [[0.02173913]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dowloading from minio my tran and test sets\n",
    "\n",
    "def download_numpy_from_minio(client, bucket_name, object_name):\n",
    "    \"\"\"\n",
    "    Download a numpy array from MinIO directly into memory.\n",
    "    Returns the loaded numpy array.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.get_object(bucket_name, object_name)\n",
    "        data = response.read()  # Read bytes\n",
    "        array = np.load(io.BytesIO(data))\n",
    "        response.close()\n",
    "        response.release_conn()\n",
    "        print(f\"Downloaded from minio://{bucket_name}/{object_name} (shape: {array.shape})\")\n",
    "        return array\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {object_name} from MinIO: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test download each array\n",
    "\n",
    "bucket_name = \"k8s-resources-forecast\"\n",
    "\n",
    "# These are your desired object paths for each array\n",
    "object_names = {\n",
    "    \"X_train\": \"data/k8s-preprocessed/node-1-X_train/X_train.npy\",\n",
    "    \"y_train\": \"data/k8s-preprocessed/node-1-y_train/y_train.npy\",\n",
    "    \"X_test\":  \"data/k8s-preprocessed/node-1-X_test/X_test.npy\",\n",
    "    \"y_test\":  \"data/k8s-preprocessed/node-1-y_test/y_test.npy\",\n",
    "}\n",
    "\n",
    "# test= val\n",
    "X_train = download_numpy_from_minio(minio_client, bucket_name, object_names[\"X_train\"])\n",
    "y_train = download_numpy_from_minio(minio_client, bucket_name, object_names[\"y_train\"])\n",
    "X_val  = download_numpy_from_minio(minio_client, bucket_name, object_names[\"X_test\"])\n",
    "y_val  = download_numpy_from_minio(minio_client, bucket_name, object_names[\"y_test\"])\n",
    "\n",
    "#PyTorch DataLoaders\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "val_dataset   = TensorDataset(torch.tensor(X_val,   dtype=torch.float32), torch.tensor(y_val,   dtype=torch.float32))\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Optional: Exploring my A and  y sets\n",
    "\n",
    "print(\"X_train.shape:\", type(X_train))\n",
    "print(\"y_train.shape:\", type(y_train))\n",
    "print(\"X_val.shape:\", type(X_val))\n",
    "print(\"y_test.shape:\", type(y_val), \"\\n\")\n",
    "\n",
    "print(\"X_train.shape:\", X_train.shape)\n",
    "print(\"y_train.shape:\", y_train.shape)\n",
    "print(\"X_val.shape:\", X_val.shape)\n",
    "print(\"y_val.shape:\", y_val.shape)\n",
    "\n",
    "# Let's see the first few samples (flatten to 1D for readability)\n",
    "print(\"\\nFirst training input window (X_train[0]):\", X_train[0].flatten())\n",
    "print(\"First training target (y_train[0]):\", y_train[0].flatten())\n",
    "\n",
    "print(\"\\nLast training input window (X_train[-1]):\", X_train[-1].flatten())\n",
    "print(\"Last training target (y_train[-1]):\", y_train[-1].flatten())\n",
    "\n",
    "print(\"\\nFirst test input window (X_val[0]):\", X_val[0].flatten())\n",
    "print(\"First test target (y_val[0]):\", y_val[0].flatten())\n",
    "\n",
    "print(\"\\nLast test input window (X_val[-1]):\", X_val[-1].flatten())\n",
    "print(\"Last test target (y_val[-1]):\", y_val[-1].flatten())\n",
    "\n",
    "print(\"First 5 input windows (X_val):\")\n",
    "display(X_val)\n",
    "print(\"\\nCorresponding targets (y_val):\")\n",
    "display(y_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafa3482-c334-4caf-85d6-7836f4193591",
   "metadata": {},
   "source": [
    "# 5. Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b989b5cb-5dab-4a35-80ee-e094308c74a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    hidden_size=64, num_layers=2, dropout=0.0, lr=0.001,\n",
    "    batch_size=32, epochs=100, patience=8, verbose=True\n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = LSTMForecaster(hidden_size=hidden_size, num_layers=num_layers, dropout=dropout).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "    val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    early_stopper = EarlyStopping(patience=patience)\n",
    "    history = {\"train_loss\": [], \"val_loss\": []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # --- Train ---\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(xb)\n",
    "            loss = criterion(output, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * xb.size(0)\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                output = model(xb)\n",
    "                loss = criterion(output, yb)\n",
    "                val_loss += loss.item() * xb.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        \n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch+1:3d} | Train Loss: {train_loss:.5f} | Val Loss: {val_loss:.5f}\")\n",
    "        \n",
    "        early_stopper(val_loss)\n",
    "        if early_stopper.early_stop:\n",
    "            if verbose: print(\"Early stopping!\")\n",
    "            break\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96d8e76-1af9-41c8-ac93-0a4a4ab249a4",
   "metadata": {},
   "source": [
    "# 6 Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "705725ff-befe-4c02-bc48-eb60d0f7e87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(y_true, y_pred):\n",
    "    y_pred = y_pred.flatten()\n",
    "    y_true = y_true.flatten()\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2}\n",
    "\n",
    "def plot_results(y_true, y_pred, history, out_prefix=\"\"):\n",
    "    # True vs Predicted\n",
    "    plt.figure(figsize=(12,3))\n",
    "    plt.plot(y_true, label=\"True\", alpha=0.7)\n",
    "    plt.plot(y_pred, label=\"Pred\", alpha=0.7)\n",
    "    plt.title(\"True vs. Predicted CPU%\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{out_prefix}true_vs_pred.png\"); plt.close()\n",
    "    \n",
    "    # Residuals\n",
    "    plt.figure(figsize=(12,3))\n",
    "    plt.plot(y_pred - y_true)\n",
    "    plt.title(\"Residuals: Prediction Error Over Time\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{out_prefix}residuals.png\"); plt.close()\n",
    "    \n",
    "    # Learning Curve\n",
    "    plt.figure(figsize=(8,3))\n",
    "    plt.plot(history[\"train_loss\"], label=\"Train\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"Val\")\n",
    "    plt.title(\"Learning Curve\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss (MSE)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{out_prefix}learning_curve.png\"); plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0aee270-e1e7-4be8-9c2f-efb0980c8c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 MLflow-Tracked Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3be73e76-005a-4b51-81e2-109e9877480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_track(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    hidden_size=64, num_layers=2, dropout=0.0, lr=0.001,\n",
    "    batch_size=32, epochs=40, patience=6,\n",
    "    window_size=5, horizon=1\n",
    "):\n",
    "    with mlflow.start_run() as run:\n",
    "        # Log all HP\n",
    "        mlflow.log_params({\n",
    "            \"hidden_size\": hidden_size, \"num_layers\": num_layers, \"dropout\": dropout,\n",
    "            \"lr\": lr, \"batch_size\": batch_size, \"epochs\": epochs,\n",
    "            \"window_size\": window_size, \"horizon\": horizon\n",
    "        })\n",
    "        # Train\n",
    "        model, history = train_lstm(\n",
    "            X_train, y_train, X_val, y_val,\n",
    "            hidden_size, num_layers, dropout, lr,\n",
    "            batch_size, epochs, patience\n",
    "        )\n",
    "        # Predict (CORRECTED SECTION)\n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        device = next(model.parameters()).device\n",
    "        val_dataset = TensorDataset(\n",
    "            torch.tensor(X_val, dtype=torch.float32),\n",
    "            torch.tensor(y_val, dtype=torch.float32)\n",
    "        )\n",
    "        val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(device)\n",
    "                preds = model(xb).cpu().numpy()\n",
    "                y_pred.append(preds)\n",
    "                y_true.append(yb.numpy())\n",
    "        y_pred = np.vstack(y_pred).squeeze()\n",
    "        y_true = np.vstack(y_true).squeeze()\n",
    "        # Metrics\n",
    "        metrics = calc_metrics(y_true, y_pred)\n",
    "        mlflow.log_metrics(metrics)\n",
    "        # Plots\n",
    "        plot_results(y_true, y_pred, history, out_prefix=\"mlflow_\")\n",
    "        mlflow.log_artifact(\"mlflow_true_vs_pred.png\")\n",
    "        mlflow.log_artifact(\"mlflow_residuals.png\")\n",
    "        mlflow.log_artifact(\"mlflow_learning_curve.png\")\n",
    "        # Save model\n",
    "        model_path = \"lstm_model.pt\"\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        mlflow.log_artifact(model_path)\n",
    "        # Register Model\n",
    "        REGISTERED_MODEL_NAME = \"k8s-cpu-forecast-lstm\"\n",
    "        result = mlflow.pytorch.log_model(model, artifact_path=\"model\", registered_model_name=REGISTERED_MODEL_NAME)\n",
    "        print(\"MLflow run:\", run.info.run_id)\n",
    "        return run.info.run_id, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da9f0b6d-e067-447a-9eb0-486c15bf9dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 | Train Loss: 0.02373 | Val Loss: 0.00113\n",
      "Epoch   2 | Train Loss: 0.00562 | Val Loss: 0.00076\n",
      "Epoch   3 | Train Loss: 0.00548 | Val Loss: 0.00061\n",
      "Epoch   4 | Train Loss: 0.00521 | Val Loss: 0.00100\n",
      "Epoch   5 | Train Loss: 0.00507 | Val Loss: 0.00111\n",
      "Epoch   6 | Train Loss: 0.00472 | Val Loss: 0.00059\n",
      "Epoch   7 | Train Loss: 0.00432 | Val Loss: 0.00062\n",
      "Epoch   8 | Train Loss: 0.00421 | Val Loss: 0.00072\n",
      "Epoch   9 | Train Loss: 0.00413 | Val Loss: 0.00078\n",
      "Epoch  10 | Train Loss: 0.00410 | Val Loss: 0.00069\n",
      "Epoch  11 | Train Loss: 0.00396 | Val Loss: 0.00067\n",
      "Early stopping!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/04 13:39:19 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cpu) contains a local version label (+cpu). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/09/04 13:39:22 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cpu) contains a local version label (+cpu). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "\u001b[31m2025/09/04 13:39:22 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n",
      "Registered model 'k8s-cpu-forecast-lstm' already exists. Creating a new version of this model...\n",
      "2025/09/04 13:39:23 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: k8s-cpu-forecast-lstm, version 3\n",
      "Created version '3' of model 'k8s-cpu-forecast-lstm'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow run: fdb2407ad84b423a904979036d8374e3\n",
      "🏃 View run likeable-fox-184 at: http://sunrise-mlflow-tracking.mlflow.svc.cluster.local:5080/#/experiments/27/runs/fdb2407ad84b423a904979036d8374e3\n",
      "🧪 View experiment at: http://sunrise-mlflow-tracking.mlflow.svc.cluster.local:5080/#/experiments/27\n",
      "{'MAE': 0.02034158632159233, 'RMSE': 0.025939794427791898, 'R2': 0.7101677656173706}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "run_id, metrics = run_and_track(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    hidden_size=64, num_layers=2, dropout=0.1, lr=0.001,\n",
    "    batch_size=32, epochs=30, patience=5,\n",
    "    window_size=5, horizon=1\n",
    ")\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c4eb223-90e3-491c-b3da-880740097a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.21.3\n"
     ]
    }
   ],
   "source": [
    "print(mlflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbb7424-68b6-4955-8b9d-e0d910f8a057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fec4b92a-8162-42de-9ae6-24462798caeb",
   "metadata": {},
   "source": [
    "# Second version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3896f349-5025-48c2-b8a8-d7111b8c55cf",
   "metadata": {},
   "source": [
    "## 1. Imports & MinIO Download Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33d6c934-ab5a-42cc-937d-e62bc1047f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import io\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Fucntion to download numpy arrays from MinIO\n",
    "def download_numpy_from_minio(minio_client, bucket, object_name):\n",
    "    try:\n",
    "        with minio_client.get_object(bucket, object_name) as response:\n",
    "            arr = np.load(io.BytesIO(response.read()))\n",
    "            print(f\"Downloaded: s3://{bucket}/{object_name} shape={arr.shape}\")\n",
    "            return arr\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07f7b26-2ca0-480f-9a4d-879a63cc4c68",
   "metadata": {},
   "source": [
    "## 2: Load Train/Val Sets From MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f8496232-f529-45f1-99b0-80105b12e5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded from minio://k8s-resources-forecast/data/k8s-preprocessed/node-1-X_train/X_train.npy (shape: (8238, 5, 1))\n",
      "Downloaded from minio://k8s-resources-forecast/data/k8s-preprocessed/node-1-y_train/y_train.npy (shape: (8238, 1, 1))\n",
      "Downloaded from minio://k8s-resources-forecast/data/k8s-preprocessed/node-1-X_test/X_test.npy (shape: (2060, 5, 1))\n",
      "Downloaded from minio://k8s-resources-forecast/data/k8s-preprocessed/node-1-y_test/y_test.npy (shape: (2060, 1, 1))\n",
      "X_train shape: (8238, 5, 1) y_train shape: (8238, 1, 1)\n",
      "X_val shape: (2060, 5, 1) y_val shape: (2060, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "bucket_name = \"k8s-resources-forecast\"\n",
    "object_names = {\n",
    "    \"X_train\": \"data/k8s-preprocessed/node-1-X_train/X_train.npy\",\n",
    "    \"y_train\": \"data/k8s-preprocessed/node-1-y_train/y_train.npy\",\n",
    "    \"X_val\":   \"data/k8s-preprocessed/node-1-X_test/X_test.npy\",\n",
    "    \"y_val\":   \"data/k8s-preprocessed/node-1-y_test/y_test.npy\",\n",
    "}\n",
    "\n",
    "X_train = download_numpy_from_minio(minio_client, bucket_name, object_names[\"X_train\"])\n",
    "y_train = download_numpy_from_minio(minio_client, bucket_name, object_names[\"y_train\"])\n",
    "X_val   = download_numpy_from_minio(minio_client, bucket_name, object_names[\"X_val\"])\n",
    "y_val   = download_numpy_from_minio(minio_client, bucket_name, object_names[\"y_val\"])\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape, \"y_train shape:\", y_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape, \"y_val shape:\", y_val.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c136bb-477e-4066-8bf8-1f346d9b76e4",
   "metadata": {},
   "source": [
    "## 3: Build PyTorch DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3e14b017-4f8a-4dd1-9c08-460cb8609f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
    "                              torch.tensor(y_train, dtype=torch.float32))\n",
    "val_dataset   = TensorDataset(torch.tensor(X_val,   dtype=torch.float32),\n",
    "                              torch.tensor(y_val,   dtype=torch.float32))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9633c3-d748-4c8d-9a59-ed534d9ce44e",
   "metadata": {},
   "source": [
    "## 4: Define LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f935173-8905-45c3-b5a5-b90c68ec695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMForecaster(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, output_size=1, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out.unsqueeze(1)  # (batch, horizon, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd9f413-6520-4537-8e21-a0398ebb31ea",
   "metadata": {},
   "source": [
    "## 5: Training Loop With Early Stopping and MLflow Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e71d9bc-4fc1-40ec-bdf4-73ac4f23d842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_early_stopping(\n",
    "    train_loader, val_loader, input_size=1, hidden_size=64, num_layers=2,\n",
    "    lr=0.001, epochs=35, patience=5, dropout=0.0, model_name=\"cpu-pct\", run_name=None\n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = LSTMForecaster(input_size, hidden_size, num_layers, output_size=1, dropout=dropout).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    wait = 0\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        mlflow.log_params({\n",
    "            \"input_size\": input_size, \"hidden_size\": hidden_size,\n",
    "            \"num_layers\": num_layers, \"lr\": lr, \"epochs\": epochs,\n",
    "            \"batch_size\": BATCH_SIZE, \"dropout\": dropout, \"patience\": patience\n",
    "        })\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            running_loss = 0\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                out = model(xb)\n",
    "                loss = criterion(out, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            train_loss = running_loss / len(train_loader)\n",
    "            train_losses.append(train_loss)\n",
    "\n",
    "            model.eval()\n",
    "            val_running_loss = 0\n",
    "            all_pred, all_true = [], []\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_loader:\n",
    "                    xb, yb = xb.to(device), yb.to(device)\n",
    "                    out = model(xb)\n",
    "                    loss = criterion(out, yb)\n",
    "                    val_running_loss += loss.item()\n",
    "                    all_pred.append(out.cpu().numpy())\n",
    "                    all_true.append(yb.cpu().numpy())\n",
    "            val_loss = val_running_loss / len(val_loader)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.5f} | Val Loss: {val_loss:.5f}\")\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model = model.state_dict()\n",
    "                wait = 0\n",
    "            else:\n",
    "                wait += 1\n",
    "                if wait >= patience:\n",
    "                    mlflow.log_metric(\"epoch_actual\", epoch + 1)  # real epoch that run\n",
    "                    print(\"Early stopping triggered!\")\n",
    "                    break\n",
    "\n",
    "        # Load best\n",
    "        if best_model: model.load_state_dict(best_model)\n",
    "\n",
    "        # Final metrics\n",
    "        model.eval()\n",
    "        preds, targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                out = model(xb.to(device)).cpu().numpy()\n",
    "                preds.append(out)\n",
    "                targets.append(yb.cpu().numpy())\n",
    "        preds = np.concatenate(preds).reshape(-1)\n",
    "        targets = np.concatenate(targets).reshape(-1)\n",
    "\n",
    "        mae  = mean_absolute_error(targets, preds)\n",
    "        #rmse = mean_squared_error(targets, preds, squared=False) #  scikit-learn new version has this\n",
    "        rmse = np.sqrt(mean_squared_error(targets, preds))\n",
    "        r2   = r2_score(targets, preds)\n",
    "        mlflow.log_metric(\"val_mae\", mae)\n",
    "        mlflow.log_metric(\"val_rmse\", rmse)\n",
    "        mlflow.log_metric(\"val_r2\", r2)\n",
    "\n",
    "        # --- Plots\n",
    "        plt.figure(figsize=(10,4))\n",
    "        plt.plot(targets, label=\"True\")\n",
    "        plt.plot(preds, label=\"Predicted\")\n",
    "        plt.legend(); plt.title(\"True vs. Predicted CPU% (Validation)\")\n",
    "        plt.tight_layout(); plt.savefig(\"true_vs_pred.png\"); plt.close()\n",
    "        mlflow.log_artifact(\"true_vs_pred.png\")\n",
    "\n",
    "        plt.figure(figsize=(10,4))\n",
    "        plt.plot(preds - targets)\n",
    "        plt.title(\"Residuals Over Time\"); plt.xlabel(\"Time\"); plt.ylabel(\"Residual (Pred - True)\")\n",
    "        plt.tight_layout(); plt.savefig(\"residuals.png\"); plt.close()\n",
    "        mlflow.log_artifact(\"residuals.png\")\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(train_losses, label=\"Train Loss\")\n",
    "        plt.plot(val_losses, label=\"Val Loss\")\n",
    "        plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Learning Curve\")\n",
    "        plt.legend(); plt.tight_layout(); plt.savefig(\"learning_curve.png\"); plt.close()\n",
    "        mlflow.log_artifact(\"learning_curve.png\")\n",
    "\n",
    "        mlflow.pytorch.log_model(model, \"model\")\n",
    "        print(\"Model + artifacts logged in MLflow.\")\n",
    "\n",
    "    return model, (mae, rmse, r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a27d03f-4183-4d78-8edc-2c2e708a652c",
   "metadata": {},
   "source": [
    "## 6: Run the Training & Logging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7af92d67-3fc6-4cfa-bdba-27fa6422805f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35 | Train Loss: 0.02433 | Val Loss: 0.00150\n",
      "Epoch 2/35 | Train Loss: 0.00557 | Val Loss: 0.00070\n",
      "Epoch 3/35 | Train Loss: 0.00548 | Val Loss: 0.00065\n",
      "Epoch 4/35 | Train Loss: 0.00524 | Val Loss: 0.00059\n",
      "Epoch 5/35 | Train Loss: 0.00507 | Val Loss: 0.00087\n",
      "Epoch 6/35 | Train Loss: 0.00504 | Val Loss: 0.00086\n",
      "Epoch 7/35 | Train Loss: 0.00475 | Val Loss: 0.00098\n",
      "Epoch 8/35 | Train Loss: 0.00444 | Val Loss: 0.00083\n",
      "Epoch 9/35 | Train Loss: 0.00450 | Val Loss: 0.00062\n",
      "Early stopping triggered!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/04 15:52:56 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cpu) contains a local version label (+cpu). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/09/04 15:52:58 WARNING mlflow.utils.requirements_utils: Found torch version (2.8.0+cpu) contains a local version label (+cpu). MLflow logged a pip requirement for this package as 'torch==2.8.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "\u001b[31m2025/09/04 15:52:58 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model + artifacts logged in MLflow.\n",
      "🏃 View run lstm-early-stop at: http://sunrise-mlflow-tracking.mlflow.svc.cluster.local:5080/#/experiments/27/runs/54fc01de8c3f4ea9839f4d5deb32bca0\n",
      "🧪 View experiment at: http://sunrise-mlflow-tracking.mlflow.svc.cluster.local:5080/#/experiments/27\n",
      "Final MAE: 0.0195 | RMSE: 0.0248 | R2: 0.7353\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 35\n",
    "PATIENCE = 5\n",
    "\n",
    "model, metrics = train_model_with_early_stopping(\n",
    "    train_loader, val_loader,\n",
    "    input_size=X_train.shape[-1],\n",
    "    hidden_size=64,\n",
    "    num_layers=2,\n",
    "    lr=0.001,\n",
    "    epochs=EPOCHS,\n",
    "    patience=PATIENCE,\n",
    "    dropout=0.1,\n",
    "    model_name=\"cpu-pct\",\n",
    "    run_name=\"lstm-early-stop\"\n",
    ")\n",
    "\n",
    "print(f\"Final MAE: {metrics[0]:.4f} | RMSE: {metrics[1]:.4f} | R2: {metrics[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309473c3-d7fa-4d4e-86b4-b9de147ee3e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
