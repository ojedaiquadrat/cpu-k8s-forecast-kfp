{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5db65d8-5bcf-429e-aa42-db0c6183db0e",
   "metadata": {},
   "source": [
    "# 1. Exploring the MlFlow Storage to deploy the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34884aa6-73a9-416d-a1cc-6f80574e62f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://sunrise-mlflow-tracking.mlflow.svc.cluster.local:5080\")\n",
    "client = MlflowClient()\n",
    "\n",
    "mv = client.get_model_version(\"cpu-pct-test-5\", \"1\")   # <- pick your model/version\n",
    "print(mv.source)  # e.g. s3://mlflow/27/3e0850.../artifacts/model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9f400f-abab-4b1b-b85a-54e2439575f0",
   "metadata": {},
   "source": [
    "### Minio fro Mlflow credential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4893968d-fbeb-4c19-b5a5-2e6842d63d4a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "kubectl -n mlflow get secrets | grep -i minio\n",
    "\n",
    "# Example for Bitnami chart:\n",
    "kubectl -n mlflow get secret sunrise-minio -o jsonpath='{.data.root-user}' | base64 -d; echo\n",
    "kubectl -n mlflow get secret sunrise-minio -o jsonpath='{.data.root-password}' | base64 -d; echo\n",
    "\n",
    "'''\n",
    "\n",
    "sunrise-minio                   Opaque               2      123d\n",
    "\n",
    "admin                            user\n",
    "\n",
    "ySO5ISk7Eq                       password\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44e91f0-59bc-4a8e-bbcf-5784ac64f56e",
   "metadata": {},
   "source": [
    "# 2. KServe\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a8dea189-4b61-472a-8d51-ba3a1d5f966c",
   "metadata": {},
   "source": [
    "eduardo@iquPC:training-pipeline$ # Show all containers & images in the KServe controller deployment\n",
    "kubectl -n kserve get deploy kserve-controller-manager \\\n",
    "  -o jsonpath='{range .spec.template.spec.containers[*]}{.name}{\" => \"}{.image}{\"\\n\"}{end}'\n",
    "\n",
    "# Print only the manager container image (this is your KServe version tag)\n",
    "kubectl -n kserve get deploy kserve-controller-manager \\\n",
    "  -o jsonpath='{.spec.template.spec.containers[?(@.name==\"manager\")].image}{\"\\n\"}'\n",
    "\n",
    "# Just the tag (strip repo/name):\n",
    "kubectl -n kserve get deploy kserve-controller-manager \\\n",
    "  -o jsonpath='{.spec.template.spec.containers[?(@.name==\"manager\")].image}' \\\n",
    "| sed -E 's#.*:([^:@]+)(@sha.*)?$#\\1#'\n",
    "kube-rbac-proxy => quay.io/brancz/kube-rbac-proxy:v0.18.0\n",
    "manager => kserve/kserve-controller:v0.15.0\n",
    "kserve/kserve-controller:v0.15.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a1ceca-d247-47d7-a51a-68bfdc968d32",
   "metadata": {},
   "source": [
    "### K server issue #1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ed5612-eed3-49e3-8de4-69fa05b7466c",
   "metadata": {},
   "source": [
    "Bingo — the ClusterRoleBindings point to the wrong namespace (kubeflow) for the kserve-controller-manager SA. That’s why every can-i came back no and the controller can’t sync caches.\n",
    "\n",
    "Let’s fix the bindings so they reference the SA in kserve.\n",
    "\n",
    "1) Recreate the two bindings with the correct subject\n",
    "\n",
    "(This is safer than JSON-patching indexes; it idempotently overwrites them.)\n",
    "\n",
    "\n",
    "**Manager -> needs cluster-wide perms**\n",
    "kubectl create clusterrolebinding kserve-manager-rolebinding \\\n",
    "  --clusterrole=kserve-manager-role \\\n",
    "  --serviceaccount=kserve:kserve-controller-manager \\\n",
    "  -o yaml --dry-run=client | kubectl apply -f -\n",
    "\n",
    "**Proxy -> used by kube-rbac-proxy sidecar**\n",
    "kubectl create clusterrolebinding kserve-proxy-rolebinding \\\n",
    "  --clusterrole=kserve-proxy-role \\\n",
    "  --serviceaccount=kserve:kserve-controller-manager \\\n",
    "  -o yaml --dry-run=client | kubectl apply -f -\n",
    "\n",
    "\n",
    "kubectl -n kserve rollout restart deploy/kserve-controller-manager\n",
    "kubectl -n kserve get pods -w\n",
    "\n",
    "**then:**\n",
    "kubectl -n kserve logs deploy/kserve-controller-manager -c manager --tail=200\n",
    "\n",
    "\n",
    "for r in \\\n",
    "  \"services\" \\\n",
    "  \"deployments.apps\" \\\n",
    "  \"virtualservices.networking.istio.io\" \\\n",
    "  \"httproutes.gateway.networking.k8s.io\" \\\n",
    "  \"inferenceservices.serving.kserve.io\" \\\n",
    "  \"trainedmodels.serving.kserve.io\" \\\n",
    "  \"inferencegraphs.serving.kserve.io\"\n",
    "do\n",
    "  printf \"%-45s \" \"$r\"; \\\n",
    "  kubectl auth can-i --as=system:serviceaccount:kserve:kserve-controller-manager list \"$r\" --all-namespaces\n",
    "done\n",
    "    \n",
    "######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616ea53c-fc4e-4401-b457-e93084d5a21b",
   "metadata": {},
   "source": [
    "### #issue 2\n",
    "\n",
    "this check the problems if it is there becasue the user \"system:serviceaccount:kubeflow:kserve-controller-manager\" cannot get resource \"serviceaccounts\" in the namespace \"lstm-iqu\"\n",
    "* kubectl -n kubeflow logs deploy/kserve-controller-manager -c manager | grep -i credential^C\n",
    "* kubectl -n kubeflow get cm inferenceservice-config -o yaml | grep -A5 credentials\n",
    "\n",
    "note: Because the KServe controller can’t read your ServiceAccount in lstm-iqu, it fails to inject S3 env vars into the storage-initializer, so you get NoCredentialsError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16364d1-d2c8-437e-afe7-1dd23583b5d6",
   "metadata": {},
   "source": [
    "The kserver cannot inject the secrets credenatial and service account in the pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690583f3-00bc-448b-9c08-d7f785f801f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3797ec5-5cd0-4e51-869c-74d178197714",
   "metadata": {},
   "source": [
    "#### CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81e795b6-aac1-4f15-b604-7898937dbeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubernetes import client\n",
    "from kserve import KServeClient\n",
    "from kserve import constants\n",
    "from kserve import V1beta1InferenceService, V1beta1InferenceServiceSpec, V1beta1PredictorSpec, V1beta1ModelSpec\n",
    "\n",
    "\n",
    "namespace = \"lstm-iqu\"\n",
    "name = \"inference-serving-kserve-cpu\"\n",
    "service_account = \"sa-private-mlflow\"\n",
    "storage_uri = \"s3://mlflow/27/7b7f44e59b8f4f348ec18bb0ca7da0c4/artifacts/model\"\n",
    "\n",
    "# Important: model_format is a dict with \"name\" (for MLflow)\n",
    "model_spec = V1beta1ModelSpec(\n",
    "    model_format={\"name\": \"mlflow\"},\n",
    "    storage_uri=storage_uri,\n",
    "    runtime=\"kserve-mlserver\"\n",
    ")\n",
    "\n",
    "predictor_spec = V1beta1PredictorSpec(\n",
    "    service_account_name=service_account,\n",
    "    model=model_spec,\n",
    ")\n",
    "\n",
    "isvc = V1beta1InferenceService(\n",
    "    api_version=\"serving.kserve.io/v1beta1\",\n",
    "    kind=\"InferenceService\",\n",
    "    metadata=client.V1ObjectMeta(\n",
    "        name=name,\n",
    "        namespace=namespace\n",
    "    ),\n",
    "    spec=V1beta1InferenceServiceSpec(\n",
    "        predictor=predictor_spec\n",
    "    )\n",
    ")\n",
    "\n",
    "kserve_client = KServeClient()\n",
    "kserve_client.create(isvc, namespace=namespace)\n",
    "\n",
    "# Optionally, wait for readiness\n",
    "kserve_client.wait_isvc_ready(name, namespace=namespace)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32a54fc-5de7-4670-8175-3f8e6a0edca8",
   "metadata": {},
   "source": [
    "# Component working with a specific uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c2b07c-0517-45f6-b15b-ce73cb8c2ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=\"docker.io/jhofydu/kpf-kserve:V1.0.0\",\n",
    "    packages_to_install=[\"kserve==0.15.0\", \"kubernetes\"]\n",
    ")\n",
    "def deploying_model( ):\n",
    "    \n",
    "    from kubernetes import client\n",
    "    from kserve import KServeClient\n",
    "    from kserve import constants\n",
    "    from kserve import V1beta1InferenceService, V1beta1InferenceServiceSpec, V1beta1PredictorSpec, V1beta1ModelSpec\n",
    "    \n",
    "    namespace = \"lstm-iqu\"\n",
    "    name = \"inference-serving-kserve-cpu\"\n",
    "    service_account = \"sa-private-mlflow\"\n",
    "    storage_uri = \"s3://mlflow/27/7b7f44e59b8f4f348ec18bb0ca7da0c4/artifacts/model\"\n",
    "    \n",
    "    # Important: model_format is a dict with \"name\" (for MLflow)\n",
    "    model_spec = V1beta1ModelSpec(\n",
    "        model_format={\"name\": \"mlflow\"},\n",
    "        storage_uri=storage_uri,\n",
    "        runtime=\"kserve-mlserver\"\n",
    "    )\n",
    "    \n",
    "    predictor_spec = V1beta1PredictorSpec(\n",
    "        service_account_name=service_account,\n",
    "        model=model_spec,\n",
    "    )\n",
    "    \n",
    "    isvc = V1beta1InferenceService(\n",
    "        api_version=\"serving.kserve.io/v1beta1\",\n",
    "        kind=\"InferenceService\",\n",
    "        metadata=client.V1ObjectMeta(\n",
    "            name=name,\n",
    "            namespace=namespace\n",
    "        ),\n",
    "        spec=V1beta1InferenceServiceSpec(\n",
    "            predictor=predictor_spec\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    kserve_client = KServeClient()\n",
    "    kserve_client.create(isvc, namespace=namespace)\n",
    "\n",
    "    # Optionally, wait for readiness\n",
    "    kserve_client.wait_isvc_ready(name, namespace=namespace)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6b1b84-04ce-4d13-8c66-62553b566813",
   "metadata": {},
   "source": [
    "# Getting a RUNID resgistred model that is tagged as winner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb52e67-2ab4-46bf-b383-df445090812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "import re\n",
    "from kubernetes import client\n",
    "from kserve import KServeClient\n",
    "from kserve import constants\n",
    "from kserve import V1beta1InferenceService, V1beta1InferenceServiceSpec, V1beta1PredictorSpec, V1beta1ModelSpec\n",
    "\n",
    "# ===== config =====\n",
    "TRACKING_URI = \"http://sunrise-mlflow-tracking.mlflow.svc.cluster.local:5080\"\n",
    "REGISTERED_MODEL_NAME = \"cpu-pct-test-5\"\n",
    "ALIAS = \"winner\"\n",
    "\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "client = MlflowClient()\n",
    "\n",
    "try:\n",
    "    # Get model version linked to alias\n",
    "    model_version = client.get_model_version_by_alias(REGISTERED_MODEL_NAME, ALIAS)\n",
    "    version = model_version.version\n",
    "    model_uri = f\"models:/{REGISTERED_MODEL_NAME}/{version}\"\n",
    "    print(f\"Model URI: {model_uri}\")\n",
    "\n",
    "    # Try to get run ID from tags\n",
    "    run_id = model_version.tags.get(\"run_id\", None)\n",
    "\n",
    "    # If not in tags, parse from source URI\n",
    "    if not run_id and model_version.source:\n",
    "        match = re.search(r\"/([0-9a-f]{32})/artifacts\", model_version.source)\n",
    "        if match:\n",
    "            run_id = match.group(1)\n",
    "\n",
    "    if run_id:\n",
    "        runs_uri = f\"runs:/{run_id}/model\"\n",
    "        print(f\"Run ID: {run_id}\")\n",
    "        print(f\"Runs URI: {runs_uri}\")\n",
    "    else:\n",
    "        print(\"⚠️  Could not extract run_id from alias 'winner'.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error retrieving alias '{ALIAS}' for model '{REGISTERED_MODEL_NAME}': {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c39781-2a09-45e4-86f4-763e46e8ef5e",
   "metadata": {},
   "source": [
    "# Combined get the URI of the winner and deploy it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "151e83f0-bae2-4cce-96fa-57f6c26417c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Storage URI: s3://mlflow/27/a4bf131840464350ad5dcfd941e7ddcf/artifacts/model\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "import re\n",
    "from kubernetes import client\n",
    "from kserve import KServeClient\n",
    "from kserve import constants\n",
    "from kserve import V1beta1InferenceService, V1beta1InferenceServiceSpec, V1beta1PredictorSpec, V1beta1ModelSpec\n",
    "\n",
    "# ===== config =====\n",
    "TRACKING_URI = \"http://sunrise-mlflow-tracking.mlflow.svc.cluster.local:5080\"\n",
    "REGISTERED_MODEL_NAME = \"cpu-pct-test-5\"\n",
    "ALIAS = \"winner\"\n",
    "\n",
    "namespace = \"lstm-iqu\"\n",
    "name = \"inference-serving-kserve-cpu\"\n",
    "service_account = \"sa-private-mlflow\"\n",
    "S3_PREFIX = \"s3://mlflow\"\n",
    "\n",
    "# ===== MLflow: Get model version + run ID =====\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow_client = MlflowClient()\n",
    "\n",
    "try:\n",
    "    model_version = mlflow_client.get_model_version_by_alias(REGISTERED_MODEL_NAME, ALIAS)\n",
    "    version = model_version.version\n",
    "    run_id = model_version.tags.get(\"run_id\")\n",
    "\n",
    "    if not run_id and model_version.source:\n",
    "        match = re.search(r\"/([0-9a-f]{32})/artifacts\", model_version.source)\n",
    "        if match:\n",
    "            run_id = match.group(1)\n",
    "\n",
    "    if not run_id:\n",
    "        raise RuntimeError(\"❌ Could not extract run_id from alias.\")\n",
    "\n",
    "    # Fetch run info to get experiment_id\n",
    "    run = mlflow_client.get_run(run_id)\n",
    "    experiment_id = run.info.experiment_id\n",
    "\n",
    "    # ✅ Correct storage URI based on experiment_id and run_id\n",
    "    storage_uri = f\"{S3_PREFIX}/{experiment_id}/{run_id}/artifacts/model\"\n",
    "    print(f\"✅ Storage URI: {storage_uri}\")\n",
    "\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"❌ Failed to resolve MLflow model: {e}\")\n",
    "\n",
    "# ===== KServe Deployment =====\n",
    "model_spec = V1beta1ModelSpec(\n",
    "    model_format={\"name\": \"mlflow\"},\n",
    "    storage_uri=storage_uri,\n",
    "    runtime=\"kserve-mlserver\"\n",
    ")\n",
    "\n",
    "predictor_spec = V1beta1PredictorSpec(\n",
    "    service_account_name=service_account,\n",
    "    model=model_spec,\n",
    ")\n",
    "\n",
    "isvc = V1beta1InferenceService(\n",
    "    api_version=\"serving.kserve.io/v1beta1\",\n",
    "    kind=\"InferenceService\",\n",
    "    metadata=client.V1ObjectMeta(\n",
    "        name=name,\n",
    "        namespace=namespace\n",
    "    ),\n",
    "    spec=V1beta1InferenceServiceSpec(\n",
    "        predictor=predictor_spec\n",
    "    )\n",
    ")\n",
    "\n",
    "kserve_client = KServeClient()\n",
    "kserve_client.create(isvc, namespace=namespace)\n",
    "\n",
    "# Optionally, wait for readiness\n",
    "kserve_client.wait_isvc_ready(name, namespace=namespace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75933b6-77fe-4c1b-9bae-4f7ad60c4500",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
